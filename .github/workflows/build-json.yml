name: Build index & images JSON

on:
  push:
    branches:
      - main
  workflow_dispatch:

env:
  POSTS_DIR: Posts
  IMAGES_DIR: Images
  INDEX_PATH: index.json
  IMAGES_JSON_PATH: images.json

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install beautifulsoup4 pyyaml python-dateutil pytz

      - name: Build index.json and images.json
        run: |
          python - <<'PY'
          import os, json, re
          from pathlib import Path
          from bs4 import BeautifulSoup
          import yaml
          from dateutil import parser as dateparser
          from datetime import datetime, timezone, timedelta

          TZ_OFFSET = timezone(timedelta(hours=2))  # UTC+2

          root = Path.cwd()
          posts_root = root / os.environ.get("POSTS_DIR", "Posts")
          images_root = root / os.environ.get("IMAGES_DIR", "Images")
          index_path = root / os.environ.get("INDEX_PATH", "index.json")
          images_json_path = root / os.environ.get("IMAGES_JSON_PATH", "images.json")

          def read_file(p):
              try:
                  return p.read_text(encoding='utf-8', errors='ignore')
              except Exception:
                  return ""

          def parse_front_matter(text):
              if text.startswith('---'):
                  parts = text.split('---', 2)
                  if len(parts) >= 3:
                      try:
                          fm = yaml.safe_load(parts[1]) or {}
                          body = parts[2]
                          return fm, body
                      except Exception:
                          return {}, text
              return {}, text

          posts = []
          images_seen = {}

          def add_image(src, source_file):
              if not src:
                  return
              src = src.strip()
              images_seen[src] = images_seen.get(src) or {'src': src, 'source_files': []}
              if source_file and source_file not in images_seen[src]['source_files']:
                  images_seen[src]['source_files'].append(source_file)

          if posts_root.exists():
              for p in sorted(posts_root.rglob('*')):
                  if p.is_file() and p.suffix.lower() in ('.md', '.markdown', '.html', '.htm'):
                      text = read_file(p)
                      meta = {}
                      body = text

                      # File modified time as fallback
                      try:
                          mtime_ts = p.stat().st_mtime
                          mtime_dt = datetime.fromtimestamp(mtime_ts, tz=TZ_OFFSET)
                      except Exception:
                          mtime_dt = datetime.now(TZ_OFFSET)

                      # Parse meta/front-matter
                      if p.suffix.lower() in ('.md', '.markdown'):
                          fm, body = parse_front_matter(text)
                          if fm:
                              meta.update(fm or {})
                      else:
                          soup = BeautifulSoup(text, 'html.parser')
                          if soup.title and soup.title.string:
                              meta['title'] = soup.title.string.strip()
                          desc = soup.find('meta', attrs={'name':'description'}) or soup.find('meta', attrs={'property':'og:description'})
                          if desc and desc.get('content'):
                              meta['excerpt'] = desc['content'].strip()
                          img = soup.find('meta', attrs={'name':'image'}) or soup.find('meta', attrs={'property':'og:image'})
                          if img and img.get('content'):
                              meta['image'] = img['content'].strip()
                          kw = soup.find('meta', attrs={'name':'keywords'})
                          if kw and kw.get('content'):
                              meta.setdefault('tags', [t.strip() for t in kw['content'].split(',') if t.strip()])

                      if not meta.get('title'):
                          m = re.search(r'^\s*#\s+(.+)$', body, re.MULTILINE)
                          meta['title'] = m.group(1).strip() if m else p.stem

                      if not meta.get('excerpt'):
                          soup2 = BeautifulSoup(body, 'html.parser')
                          ptag = soup2.find('p')
                          meta['excerpt'] = ptag.get_text(strip=True)[:300] if ptag else ''

                      if not meta.get('image'):
                          soup3 = BeautifulSoup(body, 'html.parser')
                          img_tag = soup3.find('img')
                          if img_tag and img_tag.get('src'):
                              meta['image'] = img_tag.get('src').strip()
                          else:
                              meta['image'] = None

                      # Collect images for gallery
                      soup_imgs = BeautifulSoup(body, 'html.parser')
                      for it in soup_imgs.find_all('img'):
                          src = it.get('src')
                          if src:
                              add_image(src, p.relative_to(root).as_posix())
                      soup_meta = BeautifulSoup(body, 'html.parser')
                      for meta_img in soup_meta.find_all('meta'):
                          if meta_img.get('property') in ('og:image',) or meta_img.get('name') in ('image',):
                              if meta_img.get('content'):
                                  add_image(meta_img.get('content'), p.relative_to(root).as_posix())

                      # Tags and categories
                      tags = meta.get('tags') or meta.get('tag') or meta.get('keywords') or []
                      if isinstance(tags, str):
                          tags = [t.strip() for t in re.split(r'[,\s]+', tags) if t.strip()]
                      categories = meta.get('categories') or meta.get('category') or []
                      if isinstance(categories, str):
                          categories = [t.strip() for t in re.split(r'[,\s]+', categories) if t.strip()]

                      # Reading time
                      text_only = re.sub(r'<[^>]+>', '', body)
                      text_only = re.sub(r'[`*_>#~\-]{1,}', ' ', text_only)
                      words = len(re.findall(r'\w+', text_only))
                      reading_time = max(1, int(round(words / 200))) if words > 0 else 1

                      rel = p.relative_to(root).as_posix()
                      try:
                          parent_rel = p.parent.relative_to(root).as_posix()
                      except Exception:
                          parent_rel = ""
                      if p.name.lower() in ('index.html', 'index.htm'):
                          slug = '/' + parent_rel.rstrip('/') + '/'
                          if slug == '//':
                              slug = '/'
                      else:
                          slug = '/' + rel

                      # --- Date parsing ---
                      dt_obj = None

                      # 1️⃣ Meta date
                      if 'date' in meta and meta['date']:
                          try:
                              parsed = dateparser.parse(str(meta['date']))
                              if parsed:
                                  if not parsed.tzinfo:
                                      parsed = parsed.replace(tzinfo=timezone.utc)
                                  dt_obj = parsed.astimezone(TZ_OFFSET)
                          except Exception:
                              dt_obj = None

                      # 2️⃣ <p class="post-date">
                      if not dt_obj:
                          soup_date = BeautifulSoup(body, 'html.parser')
                          pdate = soup_date.find('p', class_='post-date')
                          if pdate:
                              try:
                                  parsed = dateparser.parse(pdate.get_text(strip=True))
                                  if parsed:
                                      if not parsed.tzinfo:
                                          parsed = parsed.replace(tzinfo=timezone.utc)
                                      dt_obj = parsed.astimezone(TZ_OFFSET)
                              except Exception:
                                  dt_obj = None

                      # 3️⃣ Fallback: file modified time
                      if not dt_obj:
                          dt_obj = mtime_dt

                      # store numeric timestamp for sorting
                      posts.append({
                          'title': meta.get('title'),
                          'date': dt_obj.isoformat(),
                          'date_ts': dt_obj.timestamp(),
                          'excerpt': meta.get('excerpt', ''),
                          'image': meta.get('image'),
                          'path': slug,
                          'source': rel,
                          'tags': tags,
                          'categories': categories,
                          'reading_time': reading_time,
                          'word_count': words
                      })

          # Images folder
          if images_root.exists():
              for p in sorted(images_root.rglob('*')):
                  if p.is_file() and p.suffix.lower() in ('.png', '.jpg', '.jpeg', '.gif', '.webp', '.svg'):
                      rel = p.relative_to(root).as_posix()
                      add_image('/' + rel, rel)

          if not posts:
              posts = [{"title": "No posts yet", "excerpt": "", "path": "/", "source": ""}]

          # Sort posts by timestamp (latest on top)
          def sort_key(item):
              ts = item.get('date_ts')
              if ts is not None:
                  return ts
              return datetime.min.replace(tzinfo=TZ_OFFSET).timestamp()

          posts_sorted = sorted(posts, key=sort_key, reverse=True)

          for p in posts_sorted:
              p.pop('date_ts', None)  # clean up internal field

          images_list = [{'src': v['src'], 'source_files': v['source_files']} for v in images_seen.values()]

          index_path.write_text(json.dumps(posts_sorted, indent=2, ensure_ascii=False), encoding='utf-8')
          images_json_path.write_text(json.dumps(images_list, indent=2, ensure_ascii=False), encoding='utf-8')
          print(f"Written {index_path} with {len(posts_sorted)} posts (UTC+2).")
          print(f"Written {images_json_path} with {len(images_list)} images.")
          PY

      - name: Commit and push JSON
        run: |
          git config --global user.name "github-actions"
          git config --global user.email "github-actions@github.com"
          git add index.json images.json
          git commit -m "Update index.json and images.json [skip ci]" || echo "Nothing to commit"
          git push origin main
